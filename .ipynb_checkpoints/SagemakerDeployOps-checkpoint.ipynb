{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Deploying with the AWS SageMaker\n",
    "In order to productionize the idea of the model to the real world, AWS SageMaker is a no-match to anything. In order to deploy the model to the real world, train and deploy with the AWS SageMaker will require an engineering workflow. \n",
    "### Importing the Necessary Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Hold of the AWS SageMaker credentials, Role and Bucket.\n",
    "The Current SageMaker Session running throughout this notebook and beyond will be much required to get hold of the underlying bucket, execution role and IAM specifics and specific permissions and privillages of the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-west-2-782510500637'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading to Datasets to S3: This may take significant amount of time because of Large Size of Data.\n",
    "Clean up of the bucket mentioned above after training the model will be required not to incur additional charges on the AWS Bills or not to exceed the free tier or credits that have been applied to the AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_dir = 'data'\n",
    "train_prefix = 'train_chest_xray/train'\n",
    "test_prefix = 'test_chest_xray/test'\n",
    "#uploading both of these two to S3 for Sagemaker Inference:\n",
    "train_data = session.upload_data(os.path.join(data_dir, 'workdir'), key_prefix = train_prefix)\n",
    "test_data = session.upload_data(os.path.join(data_dir, 'testdir'), key_prefix = test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-782510500637/train_chest_xray\n",
      "s3://sagemaker-us-west-2-782510500637/test_chest_xray\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "model_prefix = 'chest_xray_model'\n",
    "chest_xray_pyt = PyTorch(role = role, \n",
    "                         entry_point='train.py',\n",
    "                         source_dir='sagemaker_scripts', \n",
    "                         train_instance_count=1,\n",
    "                         train_instance_type = 'ml.p2.xlarge', \n",
    "                         sagemaker_session = session, \n",
    "                         framework_version='0.4.0'\n",
    "                        )                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-09 21:15:35 Starting - Starting the training job...\n",
      "2020-04-09 21:15:37 Starting - Launching requested ML instances......\n",
      "2020-04-09 21:16:36 Starting - Preparing the instances for training......\n",
      "2020-04-09 21:17:48 Downloading - Downloading input data......\n",
      "2020-04-09 21:19:03 Training - Downloading the training image...\n",
      "2020-04-09 21:19:24 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:24,696 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:24,722 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:24,941 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:25,166 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:25,166 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:25,166 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:25,166 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/18/c0b937e2f84095ae230196899e56d1d7d76c8e8424fb235ed7e5bb6d68af/numpy-1.18.2-cp35-cp35m-manylinux1_x86_64.whl (20.0MB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-w90gjma6/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed numpy-1.18.2 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:30,855 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"num_gpus\": 1,\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"hyperparameters\": {},\n",
      "    \"num_cpus\": 4,\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-09-21-15-35-146/source/sourcedir.tar.gz\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-04-09-21-15-35-146\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"module_name\": \"train\",\n",
      "    \"log_level\": 20,\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-04-09-21-15-35-146\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-09-21-15-35-146/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-09-21-15-35-146/source/sourcedir.tar.gz\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 20 : Train Batch Loss: 1.3391365230083465\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 40 : Train Batch Loss: 1.3938928306102754\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 60 : Train Batch Loss: 1.4258167564868927\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 80 : Train Batch Loss: 1.338953334093094\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 100 : Train Batch Loss: 1.3928941428661346\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 120 : Train Batch Loss: 1.316988429427147\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 140 : Train Batch Loss: 1.294007706642151\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 160 : Train Batch Loss: 1.3870243012905121\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 180 : Train Batch Loss: 1.3556297302246094\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 200 : Train Batch Loss: 1.3689688801765443\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 220 : Train Batch Loss: 1.413428670167923\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 240 : Train Batch Loss: 1.3641146898269654\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 260 : Train Batch Loss: 1.400986671447754\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 20 : Train Batch Loss: 1.3405227720737458\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 40 : Train Batch Loss: 1.385640525817871\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 60 : Train Batch Loss: 1.3581058621406554\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 80 : Train Batch Loss: 1.3800692677497863\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 100 : Train Batch Loss: 1.3682833909988403\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 120 : Train Batch Loss: 1.373577493429184\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 140 : Train Batch Loss: 1.3555822402238846\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 160 : Train Batch Loss: 1.428092509508133\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 180 : Train Batch Loss: 1.366832250356674\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 200 : Train Batch Loss: 1.3136492013931274\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 220 : Train Batch Loss: 1.3406028032302857\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 240 : Train Batch Loss: 1.3902139604091643\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 260 : Train Batch Loss: 1.348087215423584\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 20 : Train Batch Loss: 1.3710266262292863\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 40 : Train Batch Loss: 1.3756777346134186\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 60 : Train Batch Loss: 1.378966462612152\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 80 : Train Batch Loss: 1.376040482521057\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 100 : Train Batch Loss: 1.2418204575777054\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 120 : Train Batch Loss: 1.350531178712845\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 140 : Train Batch Loss: 1.3987851977348327\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 160 : Train Batch Loss: 1.2953826755285263\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 180 : Train Batch Loss: 1.3713366627693175\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 200 : Train Batch Loss: 1.3837057530879975\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 220 : Train Batch Loss: 1.3584318220615388\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 240 : Train Batch Loss: 1.370333045721054\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 260 : Train Batch Loss: 1.3463095724582672\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 20 : Train Batch Loss: 1.3381896376609803\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 40 : Train Batch Loss: 1.2795114278793336\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 60 : Train Batch Loss: 1.3678632855415345\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 80 : Train Batch Loss: 1.419531264901161\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 100 : Train Batch Loss: 1.315690878033638\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 120 : Train Batch Loss: 1.389060640335083\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 140 : Train Batch Loss: 1.2933162182569504\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 160 : Train Batch Loss: 1.2722252547740935\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 180 : Train Batch Loss: 1.3657508730888366\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 200 : Train Batch Loss: 1.3244152158498763\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 220 : Train Batch Loss: 1.388076549768448\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 240 : Train Batch Loss: 1.4266132712364197\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 260 : Train Batch Loss: 1.3830569386482239\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 20 : Train Batch Loss: 1.368595403432846\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 40 : Train Batch Loss: 1.365812563896179\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 60 : Train Batch Loss: 1.3127027779817582\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 80 : Train Batch Loss: 1.4041200935840608\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 100 : Train Batch Loss: 1.3350677073001862\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 120 : Train Batch Loss: 1.3554274678230285\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 140 : Train Batch Loss: 1.339668208360672\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 160 : Train Batch Loss: 1.3366727530956268\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 180 : Train Batch Loss: 1.3817522585392\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 200 : Train Batch Loss: 1.3184704899787902\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 220 : Train Batch Loss: 1.2841948717832565\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 240 : Train Batch Loss: 1.3260877668857574\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 260 : Train Batch Loss: 1.351065182685852\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 20 : Train Batch Loss: 1.342075628042221\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 40 : Train Batch Loss: 1.353390347957611\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 60 : Train Batch Loss: 1.3435648679733276\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 80 : Train Batch Loss: 1.3646934568881988\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 100 : Train Batch Loss: 1.3976040303707122\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 120 : Train Batch Loss: 1.3827863216400147\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 140 : Train Batch Loss: 1.3239451110363007\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 160 : Train Batch Loss: 1.3544382572174072\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 180 : Train Batch Loss: 1.287216231226921\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 200 : Train Batch Loss: 1.3539917528629304\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 220 : Train Batch Loss: 1.3637051165103913\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 240 : Train Batch Loss: 1.2981665968894958\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 260 : Train Batch Loss: 1.344440108537674\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 20 : Train Batch Loss: 1.3493833780288695\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 40 : Train Batch Loss: 1.4177481949329376\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 60 : Train Batch Loss: 1.393833076953888\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 80 : Train Batch Loss: 1.3137448698282241\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 100 : Train Batch Loss: 1.3353678941726685\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 120 : Train Batch Loss: 1.3547968685626983\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 140 : Train Batch Loss: 1.2822405099868774\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 160 : Train Batch Loss: 1.3515027463436127\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 180 : Train Batch Loss: 1.395523828268051\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 200 : Train Batch Loss: 1.3161831557750703\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 220 : Train Batch Loss: 1.3406230688095093\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 240 : Train Batch Loss: 1.3025753557682038\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 260 : Train Batch Loss: 1.3791879713535309\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 20 : Train Batch Loss: 1.3144666135311127\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 40 : Train Batch Loss: 1.4396645307540894\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 60 : Train Batch Loss: 1.385741639137268\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 80 : Train Batch Loss: 1.2888895452022553\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 100 : Train Batch Loss: 1.3139932334423066\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 120 : Train Batch Loss: 1.3589564383029937\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 140 : Train Batch Loss: 1.3564688444137574\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 160 : Train Batch Loss: 1.2988328039646149\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 180 : Train Batch Loss: 1.4060220956802367\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 200 : Train Batch Loss: 1.4021614402532578\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 220 : Train Batch Loss: 1.3527521431446075\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 240 : Train Batch Loss: 1.3922982931137085\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 260 : Train Batch Loss: 1.37211731672287\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 20 : Train Batch Loss: 1.2748484849929809\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 40 : Train Batch Loss: 1.4130431056022643\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 60 : Train Batch Loss: 1.3935795903205872\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 80 : Train Batch Loss: 1.3336711764335631\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 100 : Train Batch Loss: 1.3435133785009383\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 120 : Train Batch Loss: 1.357124823331833\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 140 : Train Batch Loss: 1.3004555344581603\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 160 : Train Batch Loss: 1.3565302312374115\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 180 : Train Batch Loss: 1.3278025090694427\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 200 : Train Batch Loss: 1.3257653176784516\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 220 : Train Batch Loss: 1.3697225272655487\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 240 : Train Batch Loss: 1.2865536212921143\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 260 : Train Batch Loss: 1.316025149822235\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 20 : Train Batch Loss: 1.365851730108261\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 40 : Train Batch Loss: 1.3038647055625916\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 60 : Train Batch Loss: 1.3717842817306518\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 80 : Train Batch Loss: 1.3741516888141632\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 100 : Train Batch Loss: 1.4130537241697312\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 120 : Train Batch Loss: 1.3413811802864075\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 140 : Train Batch Loss: 1.4433269679546357\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 160 : Train Batch Loss: 1.3935459315776826\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 180 : Train Batch Loss: 1.3213283956050872\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 200 : Train Batch Loss: 1.389029425382614\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 220 : Train Batch Loss: 1.3047487020492554\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 240 : Train Batch Loss: 1.3173021733760835\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 260 : Train Batch Loss: 1.316642525792122\u001b[0m\n",
      "\n",
      "2020-04-09 22:53:51 Uploading - Uploading generated training model\u001b[34m2020-04-09 22:53:50,284 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-09 22:55:19 Completed - Training job completed\n",
      "Training seconds: 5851\n",
      "Billable seconds: 5851\n",
      "CPU times: user 9.64 s, sys: 401 ms, total: 10 s\n",
      "Wall time: 1h 40min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chest_xray_pyt.fit({'train':train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-09 22:55:19 Starting - Preparing the instances for training\n",
      "2020-04-09 22:55:19 Downloading - Downloading input data\n",
      "2020-04-09 22:55:19 Training - Training image download completed. Training in progress.\n",
      "2020-04-09 22:55:19 Uploading - Uploading generated training model\n",
      "2020-04-09 22:55:19 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:24,696 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:24,722 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:24,941 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:25,166 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:25,166 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:25,166 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:25,166 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/18/c0b937e2f84095ae230196899e56d1d7d76c8e8424fb235ed7e5bb6d68af/numpy-1.18.2-cp35-cp35m-manylinux1_x86_64.whl (20.0MB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-w90gjma6/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed numpy-1.18.2 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-04-09 21:19:30,855 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"num_gpus\": 1,\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"hyperparameters\": {},\n",
      "    \"num_cpus\": 4,\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-09-21-15-35-146/source/sourcedir.tar.gz\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-04-09-21-15-35-146\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"module_name\": \"train\",\n",
      "    \"log_level\": 20,\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-04-09-21-15-35-146\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-09-21-15-35-146/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-09-21-15-35-146/source/sourcedir.tar.gz\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 20 : Train Batch Loss: 1.3391365230083465\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 40 : Train Batch Loss: 1.3938928306102754\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 60 : Train Batch Loss: 1.4258167564868927\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 80 : Train Batch Loss: 1.338953334093094\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 100 : Train Batch Loss: 1.3928941428661346\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 120 : Train Batch Loss: 1.316988429427147\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 140 : Train Batch Loss: 1.294007706642151\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 160 : Train Batch Loss: 1.3870243012905121\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 180 : Train Batch Loss: 1.3556297302246094\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 200 : Train Batch Loss: 1.3689688801765443\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 220 : Train Batch Loss: 1.413428670167923\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 240 : Train Batch Loss: 1.3641146898269654\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 260 : Train Batch Loss: 1.400986671447754\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 20 : Train Batch Loss: 1.3405227720737458\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 40 : Train Batch Loss: 1.385640525817871\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 60 : Train Batch Loss: 1.3581058621406554\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 80 : Train Batch Loss: 1.3800692677497863\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 100 : Train Batch Loss: 1.3682833909988403\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 120 : Train Batch Loss: 1.373577493429184\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 140 : Train Batch Loss: 1.3555822402238846\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 160 : Train Batch Loss: 1.428092509508133\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 180 : Train Batch Loss: 1.366832250356674\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 200 : Train Batch Loss: 1.3136492013931274\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 220 : Train Batch Loss: 1.3406028032302857\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 240 : Train Batch Loss: 1.3902139604091643\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 260 : Train Batch Loss: 1.348087215423584\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 20 : Train Batch Loss: 1.3710266262292863\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 40 : Train Batch Loss: 1.3756777346134186\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 60 : Train Batch Loss: 1.378966462612152\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 80 : Train Batch Loss: 1.376040482521057\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 100 : Train Batch Loss: 1.2418204575777054\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 120 : Train Batch Loss: 1.350531178712845\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 140 : Train Batch Loss: 1.3987851977348327\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 160 : Train Batch Loss: 1.2953826755285263\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 180 : Train Batch Loss: 1.3713366627693175\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 200 : Train Batch Loss: 1.3837057530879975\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 220 : Train Batch Loss: 1.3584318220615388\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 240 : Train Batch Loss: 1.370333045721054\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 260 : Train Batch Loss: 1.3463095724582672\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 20 : Train Batch Loss: 1.3381896376609803\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 40 : Train Batch Loss: 1.2795114278793336\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 60 : Train Batch Loss: 1.3678632855415345\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 80 : Train Batch Loss: 1.419531264901161\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 100 : Train Batch Loss: 1.315690878033638\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 120 : Train Batch Loss: 1.389060640335083\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 140 : Train Batch Loss: 1.2933162182569504\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 160 : Train Batch Loss: 1.2722252547740935\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 180 : Train Batch Loss: 1.3657508730888366\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 200 : Train Batch Loss: 1.3244152158498763\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 220 : Train Batch Loss: 1.388076549768448\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 240 : Train Batch Loss: 1.4266132712364197\u001b[0m\n",
      "\u001b[34mEpoch 4 : Batch 260 : Train Batch Loss: 1.3830569386482239\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 20 : Train Batch Loss: 1.368595403432846\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 40 : Train Batch Loss: 1.365812563896179\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 60 : Train Batch Loss: 1.3127027779817582\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 80 : Train Batch Loss: 1.4041200935840608\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 100 : Train Batch Loss: 1.3350677073001862\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 120 : Train Batch Loss: 1.3554274678230285\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 140 : Train Batch Loss: 1.339668208360672\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 160 : Train Batch Loss: 1.3366727530956268\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 180 : Train Batch Loss: 1.3817522585392\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 200 : Train Batch Loss: 1.3184704899787902\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 220 : Train Batch Loss: 1.2841948717832565\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 240 : Train Batch Loss: 1.3260877668857574\u001b[0m\n",
      "\u001b[34mEpoch 5 : Batch 260 : Train Batch Loss: 1.351065182685852\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 20 : Train Batch Loss: 1.342075628042221\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 40 : Train Batch Loss: 1.353390347957611\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 60 : Train Batch Loss: 1.3435648679733276\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 80 : Train Batch Loss: 1.3646934568881988\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 100 : Train Batch Loss: 1.3976040303707122\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 120 : Train Batch Loss: 1.3827863216400147\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 140 : Train Batch Loss: 1.3239451110363007\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 160 : Train Batch Loss: 1.3544382572174072\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 180 : Train Batch Loss: 1.287216231226921\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 200 : Train Batch Loss: 1.3539917528629304\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 220 : Train Batch Loss: 1.3637051165103913\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 240 : Train Batch Loss: 1.2981665968894958\u001b[0m\n",
      "\u001b[34mEpoch 6 : Batch 260 : Train Batch Loss: 1.344440108537674\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 20 : Train Batch Loss: 1.3493833780288695\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 40 : Train Batch Loss: 1.4177481949329376\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 60 : Train Batch Loss: 1.393833076953888\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 80 : Train Batch Loss: 1.3137448698282241\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 100 : Train Batch Loss: 1.3353678941726685\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 120 : Train Batch Loss: 1.3547968685626983\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 140 : Train Batch Loss: 1.2822405099868774\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 160 : Train Batch Loss: 1.3515027463436127\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 180 : Train Batch Loss: 1.395523828268051\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 200 : Train Batch Loss: 1.3161831557750703\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 220 : Train Batch Loss: 1.3406230688095093\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 240 : Train Batch Loss: 1.3025753557682038\u001b[0m\n",
      "\u001b[34mEpoch 7 : Batch 260 : Train Batch Loss: 1.3791879713535309\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 20 : Train Batch Loss: 1.3144666135311127\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 40 : Train Batch Loss: 1.4396645307540894\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 60 : Train Batch Loss: 1.385741639137268\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 80 : Train Batch Loss: 1.2888895452022553\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 100 : Train Batch Loss: 1.3139932334423066\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 120 : Train Batch Loss: 1.3589564383029937\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 140 : Train Batch Loss: 1.3564688444137574\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 160 : Train Batch Loss: 1.2988328039646149\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 180 : Train Batch Loss: 1.4060220956802367\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 200 : Train Batch Loss: 1.4021614402532578\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 220 : Train Batch Loss: 1.3527521431446075\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 240 : Train Batch Loss: 1.3922982931137085\u001b[0m\n",
      "\u001b[34mEpoch 8 : Batch 260 : Train Batch Loss: 1.37211731672287\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 20 : Train Batch Loss: 1.2748484849929809\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 40 : Train Batch Loss: 1.4130431056022643\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 60 : Train Batch Loss: 1.3935795903205872\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 80 : Train Batch Loss: 1.3336711764335631\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 100 : Train Batch Loss: 1.3435133785009383\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 120 : Train Batch Loss: 1.357124823331833\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 140 : Train Batch Loss: 1.3004555344581603\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 160 : Train Batch Loss: 1.3565302312374115\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 180 : Train Batch Loss: 1.3278025090694427\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 200 : Train Batch Loss: 1.3257653176784516\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 220 : Train Batch Loss: 1.3697225272655487\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 240 : Train Batch Loss: 1.2865536212921143\u001b[0m\n",
      "\u001b[34mEpoch 9 : Batch 260 : Train Batch Loss: 1.316025149822235\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 20 : Train Batch Loss: 1.365851730108261\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 40 : Train Batch Loss: 1.3038647055625916\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 60 : Train Batch Loss: 1.3717842817306518\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 80 : Train Batch Loss: 1.3741516888141632\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 100 : Train Batch Loss: 1.4130537241697312\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 120 : Train Batch Loss: 1.3413811802864075\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 140 : Train Batch Loss: 1.4433269679546357\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 160 : Train Batch Loss: 1.3935459315776826\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 180 : Train Batch Loss: 1.3213283956050872\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 200 : Train Batch Loss: 1.389029425382614\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 220 : Train Batch Loss: 1.3047487020492554\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 240 : Train Batch Loss: 1.3173021733760835\u001b[0m\n",
      "\u001b[34mEpoch 10 : Batch 260 : Train Batch Loss: 1.316642525792122\u001b[0m\n",
      "\u001b[34m2020-04-09 22:53:50,284 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 5851\n",
      "Billable seconds: 5851\n"
     ]
    }
   ],
   "source": [
    "#if no associated training jobs are found, attach the estimator with a training job. \n",
    "chest_xray_pyt = chest_xray_pyt.attach(training_job_name='sagemaker-pytorch-2020-04-09-21-15-35-146', sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating provisions for Automatic Data Capture :\n",
    "from time import gmtime, strftime\n",
    "prefix = 'auto_data_capture'\n",
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
    "\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "endpoint_name = 'chest-xray-with-data-capt-'+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "data_capture_config = DataCaptureConfig(enable_capture=True, \n",
    "                                        sampling_percentage=70, \n",
    "                                        destination_s3_uri=s3_capture_upload_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: sagemaker-pytorch-2020-04-09-21-15-35-146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = chest_xray_pyt.deploy(initial_instance_count=1,\n",
    "                                  instance_type='ml.m4.xlarge', \n",
    "                                  endpoint_name = endpoint_name, \n",
    "                                  data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = os.listdir(os.path.join('data/testdir', 'bacterial'))[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels are: \n",
      "[0, 2, 1, 1, 2, 2, 1, 0, 2, 2, 2, 0, 0, 0, 1, 1, 2, 1, 1]\n",
      "Original Labels are: \n",
      "[1, 0, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "test_dir = 'data/testdir'\n",
    "image_transformer = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "test_data = datasets.ImageFolder(test_dir, transform=image_transformer)\n",
    "batch_size=20\n",
    "num_workers=0\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=num_workers, \n",
    "                                          shuffle=True)\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "# move model inputs to cuda, if GPU available\n",
    "predictions = []\n",
    "labels_target = []\n",
    "for i in range(len(images)-1):\n",
    "    pred = predictor.predict(images[i].unsqueeze_(0))\n",
    "    pred = pred.argmax()\n",
    "    predictions.append(pred)\n",
    "    targ = labels.data[i].item()\n",
    "    labels_target.append(targ)\n",
    "print(\"Predicted Labels are: \")\n",
    "print(predictions)\n",
    "print(\"Original Labels are: \")\n",
    "print(labels_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10526315789473684"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predictions, labels_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(5))\n",
    "class_total = list(0. for i in range(5))\n",
    "\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = vgg19(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update  test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        if i < len(target.data):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "sys.stderr.write('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "sys.stderr.write('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "im = cv2.imread(os.path.join('data/testdir/bacterial', os.listdir(os.path.join('data/testdir', 'bacterial'))[0]))\n",
    "image_transformer = transforms.Compose([transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "im = image_transformer(im)\n",
    "pred = predictor.predict(im.unsqueeze_(0))\n",
    "pred.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9dbbcf45efe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
