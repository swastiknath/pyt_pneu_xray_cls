{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Deploying with the AWS SageMaker\n",
    "In order to productionize the idea of the model to the real world, AWS SageMaker is a no-match to anything. In order to deploy the model to the real world, train and deploy with the AWS SageMaker will require an engineering workflow. \n",
    "### Importing the Necessary Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Hold of the AWS SageMaker credentials, Role and Bucket.\n",
    "The Current SageMaker Session running throughout this notebook and beyond will be much required to get hold of the underlying bucket, execution role and IAM specifics and specific permissions and privillages of the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-west-2-782510500637'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading to Datasets to S3: This may take significant amount of time because of Large Size of Data.\n",
    "Clean up of the bucket mentioned above after training the model will be required not to incur additional charges on the AWS Bills or not to exceed the free tier or credits that have been applied to the AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_dir = 'data'\n",
    "train_prefix = 'train_chest_xray/train'\n",
    "test_prefix = 'test_chest_xray/test'\n",
    "#uploading both of these two to S3 for Sagemaker Inference:\n",
    "train_data = session.upload_data(os.path.join(data_dir, 'workdir'), key_prefix = train_prefix)\n",
    "test_data = session.upload_data(os.path.join(data_dir, 'testdir'), key_prefix = test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-782510500637/train_chest_xray\n",
      "s3://sagemaker-us-west-2-782510500637/test_chest_xray\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 's3://sagemaker-us-west-2-782510500637/train_chest_xray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model with the SageMaker PyTorch Estimator:\n",
    "Now we train the model with SageMaker's PyTorch Estimator and pass in the location of the training dataset which is available via the **S3 Bucket** connected to this SageMaker Session. The url to the training data has been provided which will be downloaded in the training instance at the time of training and each of the image file will be passed through a cascade of **Torchvision Transformers** which will resize them and convert them to multidimensional tensors upon which our model can perform stochastic calculations, which will then be converted into Deterministic classification.  \n",
    "\n",
    "**The training process below will require 30-45 minutes on CUDA and might take 2-3 hours on CPU to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "model_prefix = 'chest_xray_model'\n",
    "chest_xray_pyt = PyTorch(role = role, \n",
    "                         entry_point='train.py',\n",
    "                         source_dir='sagemaker_scripts', \n",
    "                         train_instance_count=1,\n",
    "                         train_instance_type = 'ml.p2.xlarge', \n",
    "                         sagemaker_session = session, \n",
    "                         framework_version='0.4.0',\n",
    "                         hyperparameters={\n",
    "                             'epochs':3\n",
    "                         }\n",
    "                        )                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-10 21:41:20 Starting - Starting the training job...\n",
      "2020-04-10 21:41:21 Starting - Launching requested ML instances......\n",
      "2020-04-10 21:42:24 Starting - Preparing the instances for training......\n",
      "2020-04-10 21:43:44 Downloading - Downloading input data.........\n",
      "2020-04-10 21:45:14 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-04-10 21:45:34,353 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-04-10 21:45:34,378 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\n",
      "2020-04-10 21:45:36 Training - Training image download completed. Training in progress.\u001b[34m2020-04-10 21:45:40,599 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-04-10 21:45:40,817 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-04-10 21:45:40,817 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-04-10 21:45:40,817 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-04-10 21:45:40,817 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/ff/18/c0b937e2f84095ae230196899e56d1d7d76c8e8424fb235ed7e5bb6d68af/numpy-1.18.2-cp35-cp35m-manylinux1_x86_64.whl (20.0MB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nlxs3qbd/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed numpy-1.18.2 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-04-10 21:45:46,656 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"log_level\": 20,\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 3\n",
      "    },\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-04-10-21-41-19-872\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-10-21-41-19-872/source/sourcedir.tar.gz\",\n",
      "    \"num_gpus\": 1,\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"TrainingInputMode\": \"File\"\n",
      "        }\n",
      "    },\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"additional_framework_parameters\": {}\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-10-21-41-19-872/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-04-10-21-41-19-872\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-782510500637/sagemaker-pytorch-2020-04-10-21-41-19-872/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\"]\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 3\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 20 : Train Batch Loss: 1.0946566373109818\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 40 : Train Batch Loss: 0.9474476784467697\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 60 : Train Batch Loss: 0.843659770488739\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 80 : Train Batch Loss: 0.7370303630828857\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 100 : Train Batch Loss: 0.6520391330122948\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 120 : Train Batch Loss: 0.6574080571532249\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 140 : Train Batch Loss: 0.6070609331130982\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 160 : Train Batch Loss: 0.6248600378632545\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 180 : Train Batch Loss: 0.6464350715279579\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 200 : Train Batch Loss: 0.647589273750782\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 220 : Train Batch Loss: 0.5907169625163078\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 240 : Train Batch Loss: 0.6720497503876686\u001b[0m\n",
      "\u001b[34mEpoch 1 : Batch 260 : Train Batch Loss: 0.5989383026957512\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 20 : Train Batch Loss: 0.5354844585061074\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 40 : Train Batch Loss: 0.5519560724496841\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 60 : Train Batch Loss: 0.5134065710008144\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 80 : Train Batch Loss: 0.4824294276535511\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 100 : Train Batch Loss: 0.5158872961997986\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 120 : Train Batch Loss: 0.5281156651675701\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 140 : Train Batch Loss: 0.5847282499074936\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 160 : Train Batch Loss: 0.539959029853344\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 180 : Train Batch Loss: 0.4852226532995701\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 200 : Train Batch Loss: 0.5364499613642693\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 220 : Train Batch Loss: 0.4917471520602703\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 240 : Train Batch Loss: 0.48497457802295685\u001b[0m\n",
      "\u001b[34mEpoch 2 : Batch 260 : Train Batch Loss: 0.4684092327952385\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 20 : Train Batch Loss: 0.5291093558073043\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 40 : Train Batch Loss: 0.4344953432679176\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 60 : Train Batch Loss: 0.43802743405103683\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 80 : Train Batch Loss: 0.40220157280564306\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 100 : Train Batch Loss: 0.4663185976445675\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 120 : Train Batch Loss: 0.464286008477211\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 140 : Train Batch Loss: 0.4156409814953804\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 160 : Train Batch Loss: 0.4580400466918945\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 180 : Train Batch Loss: 0.4278302103281021\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 200 : Train Batch Loss: 0.42834102138876917\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 220 : Train Batch Loss: 0.4459558129310608\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 240 : Train Batch Loss: 0.4201332747936249\u001b[0m\n",
      "\u001b[34mEpoch 3 : Batch 260 : Train Batch Loss: 0.4120886765420437\u001b[0m\n",
      "\u001b[34m2020-04-10 22:14:39,985 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-10 22:14:42 Uploading - Uploading generated training model\n",
      "2020-04-10 22:16:09 Completed - Training job completed\n",
      "Training seconds: 1945\n",
      "Billable seconds: 1945\n",
      "CPU times: user 3.19 s, sys: 156 ms, total: 3.34 s\n",
      "Wall time: 35min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chest_xray_pyt.fit({'train':train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if no associated training jobs are found, attach the estimator with a training job. \n",
    "# chest_xray_pyt = chest_xray_pyt.attach(training_job_name='sagemaker-pytorch-2020-04-09-21-15-35-146', sagemaker_session=session) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating provisions for Automatic Data Capture :\n",
    "from time import gmtime, strftime\n",
    "prefix = 'auto_data_capture'\n",
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
    "\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "endpoint_name = 'chest-xray-with-data-capt-'+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "data_capture_config = DataCaptureConfig(enable_capture=True, \n",
    "                                        sampling_percentage=70, \n",
    "                                        destination_s3_uri=s3_capture_upload_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 197 ms, sys: 7.77 ms, total: 205 ms\n",
      "Wall time: 7min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = chest_xray_pyt.deploy(initial_instance_count=1,\n",
    "                                  instance_type='ml.m4.xlarge', \n",
    "                                  endpoint_name = endpoint_name, \n",
    "                                  data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Inference via the Deployed Endpoint over the Test Dataset:\n",
    "The following block of code may take significant amount of time as it goes ahead and sends out one single image at a time from the test dataset and performs inference over it and compares both of their labels and calculates overall accuracy. As there are 32 batches in total with 20 images per batch, so it's a time hungry process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is : \n",
      "0.7860780984719864\n",
      "CPU times: user 25.7 s, sys: 378 ms, total: 26 s\n",
      "Wall time: 8min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "test_dir = 'data/testdir'\n",
    "image_transformer = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "test_data = datasets.ImageFolder(test_dir, transform=image_transformer)\n",
    "batch_size=20\n",
    "num_workers=0\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=num_workers, \n",
    "                                          shuffle=True)\n",
    "dataiter = iter(test_loader)\n",
    "predictions = []\n",
    "labels_target = []\n",
    "for ii in range(len(dataiter)-1):\n",
    "    images, labels = dataiter.next()\n",
    "    for i in range(len(images)-1):\n",
    "        pred = predictor.predict(images[i].unsqueeze_(0))\n",
    "        pred = pred.argmax()\n",
    "        predictions.append(pred)\n",
    "        targ = labels.data[i].item()\n",
    "        labels_target.append(targ)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(predictions, labels_target)\n",
    "print(\"Test Accuracy is : \")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing inference over a single input image:\n",
    "In order to perfrom the inference over input images the images need to go thorough a series of transformation steps like it needs to be transformed into PIL images then it needs to be resized to 224x224 and after that it needs to be transformed into a tensor. As we feed the model endpoint the transformed image as a multidimensional tensor it returns back probabilities of the input being one of the three classes. We deterministically choose the class using **argmax**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 ms, sys: 4 ms, total: 220 ms\n",
      "Wall time: 890 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Viral Pneumonia Diagnosed"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "import cv2\n",
    "classes = ['Bacterial Pneumonia Diagnosed', 'Pneumonia Not Diagnosed', 'Viral Pneumonia Diagnosed']\n",
    "im = cv2.imread(os.path.join('data/testdir/virus', os.listdir(os.path.join('data/testdir', 'virus'))[60]))\n",
    "image_transformer = transforms.Compose([transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "im = image_transformer(im)\n",
    "pred = predictor.predict(im.unsqueeze_(0))\n",
    "sys.stderr.write(classes[pred.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
